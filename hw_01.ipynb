{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2di2xSmE7sD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root = './data/dermamnist'\n",
        "os.makedirs(root, exist_ok=True)\n",
        "\n",
        "# 1. 라이브러리 설치\n",
        "!pip install medmnist torch torchvision --quiet\n",
        "\n",
        "# 2. 라이브러리 import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from medmnist import DermaMNIST\n",
        "\n",
        "# 3. GPU 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n",
        "\n",
        "# 4. 데이터 전처리(transform)\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "])\n",
        "\n",
        "# train, validation, test 데이터셋 (224x224)\n",
        "train_set = DermaMNIST(split='train', transform=transform, download=True, size=224)\n",
        "val_set = DermaMNIST(split='val', transform=transform, download=True, size=224)\n",
        "test_set = DermaMNIST(split='test', transform=transform, download=True, size=224)\n",
        "\n",
        "# 5. DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32)\n",
        "test_loader = DataLoader(test_set, batch_size=32)\n",
        "\n",
        "\n",
        "# 6. MobileNetV2 모델 정의 및 classifier 출력 수정\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_features, 7)  # DermaMNIST는 7개 클래스\n",
        "model = model.to(device)\n",
        "\n",
        "# 7. 손실함수 / optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# 8. 학습\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    running_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device).view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        running_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = running_correct / total\n",
        "\n",
        "    # 검증\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device).view(-1)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# 9. 평가\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device).view(-1)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1)\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d47d16a",
        "outputId": "a966bd37-0eea-412f-f2ba-4e1a5e6cc371"
      },
      "source": [
        "import os\n",
        "root = './data/dermamnist'\n",
        "os.makedirs(root, exist_ok=True)\n",
        "\n",
        "# 1. 라이브러리 설치 (이미 설치했다면 건너뛰세요)\n",
        "!pip install medmnist torch torchvision --quiet\n",
        "\n",
        "# 2. 라이브러리 import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from medmnist import DermaMNIST\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Early Stopping을 위해 추가\n",
        "\n",
        "# 3. GPU 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n",
        "\n",
        "# 4. 데이터 전처리(transform) (Data Augmentation 적용)\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(224), # 이미지를 무작위로 자르고 크기 조절\n",
        "    T.RandomHorizontalFlip(), # 이미지를 무작위로 좌우 반전\n",
        "    T.RandomRotation(10),     # 이미지를 무작위로 회전\n",
        "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # 색상 조절\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "])\n",
        "\n",
        "# 검증 및 테스트 데이터셋에는 Data Augmentation을 적용하지 않음\n",
        "val_test_transform = T.Compose([\n",
        "    T.Resize(256), # 크기 조절\n",
        "    T.CenterCrop(224), # 중앙 자르기\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "])\n",
        "\n",
        "\n",
        "# train, validation, test 데이터셋 (224x224) (transform 변경)\n",
        "train_set = DermaMNIST(split='train', transform=train_transform, download=True, size=224)\n",
        "val_set = DermaMNIST(split='val', transform=val_test_transform, download=True, size=224)\n",
        "test_set = DermaMNIST(split='test', transform=val_test_transform, download=True, size=224)\n",
        "\n",
        "# 5. DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32)\n",
        "test_loader = DataLoader(test_set, batch_size=32)\n",
        "\n",
        "# 6. MobileNetV2 모델 정의 및 classifier 출력 수정 (Dropout 추가)\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "in_features = model.classifier[1].in_features\n",
        "\n",
        "# 기존 classifier를 새로운 Sequential 레이어로 대체\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2), # Dropout 레이어 추가 (p는 드롭아웃 비율, 조절 가능)\n",
        "    nn.Linear(in_features, 7)  # DermaMNIST는 7개 클래스\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# 7. 손실함수 / optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4) # Weight Decay는 기존 코드 유지\n",
        "\n",
        "# Early Stopping 및 Learning Rate Scheduler 설정\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stopping_patience = 10 # 검증 손실 개선이 10번의 epoch 동안 없을 경우 중단\n",
        "\n",
        "\n",
        "# 8. 학습 (Early Stopping 로직 추가)\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    running_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device).view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        running_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = running_correct / total\n",
        "\n",
        "    # 검증\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device).view(-1)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early Stopping 체크\n",
        "    scheduler.step(val_loss) # Learning Rate Scheduler 업데이트\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Optional: Save the best model state\n",
        "        # torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "\n",
        "# 9. 평가\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device).view(-1)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1)\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {test_correct / test_total:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda (Tesla T4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Train Loss: 0.9299, Train Acc: 0.6765 | Val Loss: 0.7707, Val Acc: 0.7238\n",
            "Epoch  2 | Train Loss: 0.8274, Train Acc: 0.7063 | Val Loss: 0.7110, Val Acc: 0.7488\n",
            "Epoch  3 | Train Loss: 0.7934, Train Acc: 0.7123 | Val Loss: 0.6823, Val Acc: 0.7537\n",
            "Epoch  4 | Train Loss: 0.7719, Train Acc: 0.7207 | Val Loss: 0.7019, Val Acc: 0.7368\n",
            "Epoch  5 | Train Loss: 0.7486, Train Acc: 0.7281 | Val Loss: 0.6782, Val Acc: 0.7468\n",
            "Epoch  6 | Train Loss: 0.7459, Train Acc: 0.7213 | Val Loss: 0.6734, Val Acc: 0.7637\n",
            "Epoch  7 | Train Loss: 0.7251, Train Acc: 0.7377 | Val Loss: 0.6535, Val Acc: 0.7707\n",
            "Epoch  8 | Train Loss: 0.7187, Train Acc: 0.7383 | Val Loss: 0.6275, Val Acc: 0.7846\n",
            "Epoch  9 | Train Loss: 0.6994, Train Acc: 0.7498 | Val Loss: 0.6181, Val Acc: 0.7707\n",
            "Epoch 10 | Train Loss: 0.6966, Train Acc: 0.7460 | Val Loss: 0.5842, Val Acc: 0.7767\n",
            "Epoch 11 | Train Loss: 0.6779, Train Acc: 0.7537 | Val Loss: 0.6072, Val Acc: 0.7727\n",
            "Epoch 12 | Train Loss: 0.6707, Train Acc: 0.7548 | Val Loss: 0.6038, Val Acc: 0.7747\n",
            "Epoch 13 | Train Loss: 0.6762, Train Acc: 0.7555 | Val Loss: 0.6035, Val Acc: 0.7797\n",
            "Epoch 14 | Train Loss: 0.6549, Train Acc: 0.7620 | Val Loss: 0.5964, Val Acc: 0.7797\n",
            "Epoch 15 | Train Loss: 0.6553, Train Acc: 0.7611 | Val Loss: 0.5457, Val Acc: 0.8076\n",
            "Epoch 16 | Train Loss: 0.6530, Train Acc: 0.7597 | Val Loss: 0.5672, Val Acc: 0.7876\n",
            "Epoch 17 | Train Loss: 0.6492, Train Acc: 0.7635 | Val Loss: 0.5539, Val Acc: 0.8056\n",
            "Epoch 18 | Train Loss: 0.6543, Train Acc: 0.7678 | Val Loss: 0.5381, Val Acc: 0.8066\n",
            "Epoch 19 | Train Loss: 0.6382, Train Acc: 0.7675 | Val Loss: 0.5416, Val Acc: 0.8156\n",
            "Epoch 20 | Train Loss: 0.6236, Train Acc: 0.7697 | Val Loss: 0.5851, Val Acc: 0.7836\n",
            "Epoch 21 | Train Loss: 0.6389, Train Acc: 0.7675 | Val Loss: 0.5761, Val Acc: 0.7876\n",
            "Epoch 22 | Train Loss: 0.6188, Train Acc: 0.7731 | Val Loss: 0.5258, Val Acc: 0.8136\n",
            "Epoch 23 | Train Loss: 0.6110, Train Acc: 0.7789 | Val Loss: 0.5265, Val Acc: 0.8116\n",
            "Epoch 24 | Train Loss: 0.6113, Train Acc: 0.7835 | Val Loss: 0.5294, Val Acc: 0.8126\n",
            "Epoch 25 | Train Loss: 0.6107, Train Acc: 0.7781 | Val Loss: 0.5645, Val Acc: 0.8056\n",
            "Epoch 26 | Train Loss: 0.6075, Train Acc: 0.7822 | Val Loss: 0.5129, Val Acc: 0.8066\n",
            "Epoch 27 | Train Loss: 0.5998, Train Acc: 0.7838 | Val Loss: 0.5501, Val Acc: 0.7956\n",
            "Epoch 28 | Train Loss: 0.6027, Train Acc: 0.7779 | Val Loss: 0.5688, Val Acc: 0.7996\n",
            "Epoch 29 | Train Loss: 0.5903, Train Acc: 0.7836 | Val Loss: 0.5100, Val Acc: 0.8225\n",
            "Epoch 30 | Train Loss: 0.5875, Train Acc: 0.7851 | Val Loss: 0.5056, Val Acc: 0.8175\n",
            "Epoch 31 | Train Loss: 0.5947, Train Acc: 0.7818 | Val Loss: 0.5171, Val Acc: 0.8225\n",
            "Epoch 32 | Train Loss: 0.5798, Train Acc: 0.7874 | Val Loss: 0.5378, Val Acc: 0.8116\n",
            "Epoch 33 | Train Loss: 0.5830, Train Acc: 0.7892 | Val Loss: 0.4761, Val Acc: 0.8315\n",
            "Epoch 34 | Train Loss: 0.5616, Train Acc: 0.7986 | Val Loss: 0.5075, Val Acc: 0.8156\n",
            "Epoch 35 | Train Loss: 0.5696, Train Acc: 0.7918 | Val Loss: 0.4922, Val Acc: 0.8195\n",
            "Epoch 36 | Train Loss: 0.5595, Train Acc: 0.7942 | Val Loss: 0.4638, Val Acc: 0.8375\n",
            "Epoch 37 | Train Loss: 0.5585, Train Acc: 0.7992 | Val Loss: 0.5308, Val Acc: 0.8066\n",
            "Epoch 38 | Train Loss: 0.5518, Train Acc: 0.7976 | Val Loss: 0.5089, Val Acc: 0.8175\n",
            "Epoch 39 | Train Loss: 0.5705, Train Acc: 0.7988 | Val Loss: 0.5875, Val Acc: 0.7946\n",
            "Epoch 40 | Train Loss: 0.5461, Train Acc: 0.8001 | Val Loss: 0.4587, Val Acc: 0.8355\n",
            "Epoch 41 | Train Loss: 0.5515, Train Acc: 0.7991 | Val Loss: 0.4589, Val Acc: 0.8295\n",
            "Epoch 42 | Train Loss: 0.5426, Train Acc: 0.8028 | Val Loss: 0.5244, Val Acc: 0.8046\n",
            "Epoch 43 | Train Loss: 0.5387, Train Acc: 0.7973 | Val Loss: 0.4846, Val Acc: 0.8185\n",
            "Epoch 44 | Train Loss: 0.5476, Train Acc: 0.8011 | Val Loss: 0.4713, Val Acc: 0.8325\n",
            "Epoch 45 | Train Loss: 0.5465, Train Acc: 0.8002 | Val Loss: 0.5815, Val Acc: 0.7986\n",
            "Epoch 46 | Train Loss: 0.5350, Train Acc: 0.8038 | Val Loss: 0.4722, Val Acc: 0.8295\n",
            "Epoch 47 | Train Loss: 0.4892, Train Acc: 0.8213 | Val Loss: 0.4176, Val Acc: 0.8495\n",
            "Epoch 48 | Train Loss: 0.4521, Train Acc: 0.8356 | Val Loss: 0.4059, Val Acc: 0.8455\n",
            "Epoch 49 | Train Loss: 0.4550, Train Acc: 0.8335 | Val Loss: 0.4032, Val Acc: 0.8445\n",
            "Epoch 50 | Train Loss: 0.4551, Train Acc: 0.8300 | Val Loss: 0.4042, Val Acc: 0.8504\n",
            "Test Accuracy: 0.8479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2436566a"
      },
      "source": [
        "# Task\n",
        "Generate Python code for a machine learning task that includes training multiple models and combining their predictions using an ensemble technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eb4561d"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train multiple independent models (e.g., different architectures, or the same architecture with different initializations/data splits).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257c8309"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to train a single model with given configurations, including data augmentation, optimizer, and scheduling, and then call this function multiple times to train different models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "964cef1d",
        "outputId": "794a048d-eaa7-4309-b092-240a0fe278ae"
      },
      "source": [
        "def train_model(config):\n",
        "    \"\"\"Trains a single model with the given configuration.\"\"\"\n",
        "\n",
        "    # Data Transforms (with augmentation for training)\n",
        "    train_transform = T.Compose([\n",
        "        T.RandomResizedCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomRotation(config['augmentation_params']['rotation']),\n",
        "        T.ColorJitter(\n",
        "            brightness=config['augmentation_params']['brightness'],\n",
        "            contrast=config['augmentation_params']['contrast'],\n",
        "            saturation=config['augmentation_params']['saturation'],\n",
        "            hue=config['augmentation_params']['hue']\n",
        "        ),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "    ])\n",
        "\n",
        "    # Datasets and DataLoaders\n",
        "    train_set = DermaMNIST(split='train', transform=train_transform, download=True, size=224)\n",
        "    val_set = DermaMNIST(split='val', transform=val_test_transform, download=True, size=224)\n",
        "    train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=config['batch_size'])\n",
        "\n",
        "    # Model Initialization\n",
        "    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1) # Use weights instead of pretrained\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=config['dropout_rate']),\n",
        "        nn.Linear(in_features, 7)\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss Function and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "    # Learning Rate Scheduler and Early Stopping\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config['scheduler_factor'], patience=config['scheduler_patience'], verbose=True)\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1, config['num_epochs'] + 1):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        running_correct = 0 # Initialize running_correct for each epoch\n",
        "        total = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device).view(-1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(1) # Get predictions\n",
        "            running_correct += (preds == labels).sum().item() # Update running_correct\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = running_correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0 # Initialize val_correct for each validation\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device).view(-1)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * imgs.size(0)\n",
        "                preds = outputs.argmax(1) # Get predictions\n",
        "                val_correct += (preds == labels).sum().item() # Update val_correct\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch:2d} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early Stopping and Scheduler Step\n",
        "        scheduler.step(val_loss)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config['early_stopping_patience']:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define configurations for multiple models\n",
        "configs = [\n",
        "    {\n",
        "        'architecture': 'mobilenet_v2',\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 1e-3,\n",
        "        'weight_decay': 1e-4,\n",
        "        'num_epochs': 30, # Reduced epochs for faster execution\n",
        "        'dropout_rate': 0.2,\n",
        "        'augmentation_params': {'rotation': 10, 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'hue': 0.1},\n",
        "        'scheduler_factor': 0.1,\n",
        "        'scheduler_patience': 5,\n",
        "        'early_stopping_patience': 10\n",
        "    },\n",
        "    {\n",
        "        'architecture': 'mobilenet_v2', # Same architecture, different initialization/hyperparameters\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 5e-4,\n",
        "        'weight_decay': 1e-3,\n",
        "        'num_epochs': 30,\n",
        "        'dropout_rate': 0.3,\n",
        "        'augmentation_params': {'rotation': 15, 'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.2},\n",
        "        'scheduler_factor': 0.2,\n",
        "        'scheduler_patience': 7,\n",
        "        'early_stopping_patience': 15\n",
        "    }\n",
        "]\n",
        "\n",
        "# Train multiple models\n",
        "trained_models = []\n",
        "for i, config in enumerate(configs):\n",
        "    print(f\"Training Model {i+1}...\")\n",
        "    model = train_model(config)\n",
        "    trained_models.append(model)\n",
        "    print(f\"Model {i+1} Training Finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Train Loss: 0.9072, Train Acc: 0.6859 | Val Loss: 0.8682, Val Acc: 0.6600\n",
            "Epoch  2 | Train Loss: 0.8182, Train Acc: 0.7003 | Val Loss: 0.7112, Val Acc: 0.7408\n",
            "Epoch  3 | Train Loss: 0.7950, Train Acc: 0.7126 | Val Loss: 0.6913, Val Acc: 0.7428\n",
            "Epoch  4 | Train Loss: 0.7640, Train Acc: 0.7150 | Val Loss: 0.7463, Val Acc: 0.7408\n",
            "Epoch  5 | Train Loss: 0.7482, Train Acc: 0.7271 | Val Loss: 0.6701, Val Acc: 0.7498\n",
            "Epoch  6 | Train Loss: 0.7233, Train Acc: 0.7337 | Val Loss: 0.6440, Val Acc: 0.7617\n",
            "Epoch  7 | Train Loss: 0.7241, Train Acc: 0.7433 | Val Loss: 0.6024, Val Acc: 0.7757\n",
            "Epoch  8 | Train Loss: 0.7035, Train Acc: 0.7427 | Val Loss: 0.6284, Val Acc: 0.7797\n",
            "Epoch  9 | Train Loss: 0.6898, Train Acc: 0.7464 | Val Loss: 0.5813, Val Acc: 0.7916\n",
            "Epoch 10 | Train Loss: 0.6699, Train Acc: 0.7624 | Val Loss: 0.5522, Val Acc: 0.8086\n",
            "Epoch 11 | Train Loss: 0.6671, Train Acc: 0.7570 | Val Loss: 0.6141, Val Acc: 0.7707\n",
            "Epoch 12 | Train Loss: 0.6626, Train Acc: 0.7602 | Val Loss: 0.5840, Val Acc: 0.7936\n",
            "Epoch 13 | Train Loss: 0.6515, Train Acc: 0.7669 | Val Loss: 0.5416, Val Acc: 0.8116\n",
            "Epoch 14 | Train Loss: 0.6463, Train Acc: 0.7640 | Val Loss: 0.6419, Val Acc: 0.7737\n",
            "Epoch 15 | Train Loss: 0.6517, Train Acc: 0.7604 | Val Loss: 0.5456, Val Acc: 0.8175\n",
            "Epoch 16 | Train Loss: 0.6332, Train Acc: 0.7705 | Val Loss: 0.5415, Val Acc: 0.8046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-4-1626107124.py\", line 143, in <cell line: 0>\n",
            "    model = train_model(config)\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4-1626107124.py\", line 57, in train_model\n",
            "    for imgs, labels in train_loader:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/medmnist/dataset.py\", line 144, in __getitem__\n",
            "    img = self.transform(img)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
            "    img = t(img)\n",
            "          ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 1280, in forward\n",
            "    img = F.adjust_hue(img, hue_factor)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\", line 968, in adjust_hue\n",
            "    return F_pil.adjust_hue(img, hue_factor)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\", line 109, in adjust_hue\n",
            "    h, s, v = img.convert(\"HSV\").split()\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 1141, in convert\n",
            "    im = self.im.convert(mode, dither)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
            "    lines = linecache.getlines(file, globals_dict)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 398, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 367, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 325, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-1626107124.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Model {i+1}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-1626107124.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medmnist/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdither\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}