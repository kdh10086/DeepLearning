{"cells":[{"cell_type":"code","execution_count":null,"id":"10a5f8fb-1be8-4643-9764-ac4b3bb002c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10a5f8fb-1be8-4643-9764-ac4b3bb002c5","outputId":"f930b0d6-21dd-4d0b-ed30-14b2303659ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda (Tesla T4)\n","\n","--- Training with Learning Rate: 0.1 ---\n","Hyperparameters:\n","  Learning Rate: 0.1\n","  Number of Epochs: 50\n","  Batch Size: 32\n","  Optimizer: Adam\n","  Loss Function: CrossEntropyLoss\n","------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch  1 | Train Loss: 1.5579, Train Acc: 0.6601 | Val Loss: 1.0370, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  2 | Train Loss: 0.9888, Train Acc: 0.6680 | Val Loss: 1.2599, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  3 | Train Loss: 0.9761, Train Acc: 0.6692 | Val Loss: 1.1351, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  4 | Train Loss: 0.9982, Train Acc: 0.6686 | Val Loss: 0.9959, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  5 | Train Loss: 0.9797, Train Acc: 0.6695 | Val Loss: 1.1571, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  6 | Train Loss: 0.9861, Train Acc: 0.6686 | Val Loss: 1.1220, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  7 | Train Loss: 0.9840, Train Acc: 0.6689 | Val Loss: 0.9759, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  8 | Train Loss: 0.9913, Train Acc: 0.6665 | Val Loss: 0.9990, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  9 | Train Loss: 0.9809, Train Acc: 0.6692 | Val Loss: 3.3755, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Train Loss: 1.0328, Train Acc: 0.6659 | Val Loss: 1.0979, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11 | Train Loss: 1.0112, Train Acc: 0.6686 | Val Loss: 1.1663, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12 | Train Loss: 1.0241, Train Acc: 0.6652 | Val Loss: 1.0383, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13 | Train Loss: 1.0249, Train Acc: 0.6632 | Val Loss: 1.0415, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14 | Train Loss: 1.0417, Train Acc: 0.6659 | Val Loss: 1.2306, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15 | Train Loss: 1.0243, Train Acc: 0.6672 | Val Loss: 1.0191, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16 | Train Loss: 1.1070, Train Acc: 0.6682 | Val Loss: 1.1551, Val Acc: 0.6690\n"]},{"output_type":"stream","name":"stderr","text":[]}],"source":["import os\n","root = './data/dermamnist'\n","os.makedirs(root, exist_ok=True)\n","\n","# 1. 라이브러리 설치\n","!pip install medmnist torch torchvision tqdm --quiet # tqdm 추가 설치\n","\n","# 2. 라이브러리 import\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as T\n","from torchvision import models\n","from medmnist import DermaMNIST\n","from tqdm import tqdm # tqdm 임포트\n","\n","# 3. GPU 디바이스 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n","\n","# 4. 데이터 전처리(transform)\n","transform = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n","])\n","\n","# train, validation, test 데이터셋 (224x224)\n","train_set = DermaMNIST(split='train', transform=transform, download=True, size=224)\n","val_set = DermaMNIST(split='val', transform=transform, download=True, size=224)\n","test_set = DermaMNIST(split='test', transform=transform, download=True, size=224)\n","\n","# 5. DataLoader\n","train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=32)\n","test_loader = DataLoader(test_set, batch_size=32)\n","\n","# 학습 손실 데이터를 저장할 딕셔너리 초기화 (여러 학습률 실험을 위해 여기에 초기화)\n","training_loss_data = {}\n","\n","# 실험할 학습률 리스트\n","learning_rates_to_experiment = [1e-1, 1e-2, 1e-3, 1e-4]\n","num_epochs = 50 # 각 학습률에 대한 에포크 수\n","batch_size = train_loader.batch_size # 사용된 배치 크기\n","\n","for learning_rate in learning_rates_to_experiment:\n","    print(f\"\\n--- Training with Learning Rate: {learning_rate} ---\")\n","    # 현재 하이퍼파라미터 정보 출력\n","    print(f\"Hyperparameters:\")\n","    print(f\"  Learning Rate: {learning_rate}\")\n","    print(f\"  Number of Epochs: {num_epochs}\")\n","    print(f\"  Batch Size: {batch_size}\")\n","    print(f\"  Optimizer: Adam\") # 사용된 옵티마이저 정보\n","    print(f\"  Loss Function: CrossEntropyLoss\") # 사용된 손실 함수 정보\n","    print(\"-\" * 30)\n","\n","\n","    # 6. MobileNetV2 모델 정의 및 classifier 출력 수정 (각 학습률마다 새로 정의)\n","    # 사전 학습된 가중치를 사용하므로, 각 실험마다 모델을 새로 로드합니다.\n","    model = models.mobilenet_v2(pretrained=True)\n","    in_features = model.classifier[1].in_features\n","    model.classifier[1] = nn.Linear(in_features, 7)  # DermaMNIST는 7개 클래스\n","    model = model.to(device)\n","\n","    # 7. 손실함수 / optimizer (각 학습률마다 새로 정의)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","\n","    # 현재 학습률에 대한 손실 리스트 초기화\n","    training_loss_data[learning_rate] = []\n","\n","    # 8. 학습\n","    for epoch in range(1, num_epochs + 1):\n","        model.train()\n","        running_loss = 0\n","        running_correct = 0\n","        total = 0\n","\n","        # tqdm을 사용하여 학습 배치의 진행 상태 표시\n","        train_loop = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch}/{num_epochs} (LR: {learning_rate})\")\n","        for imgs, labels in train_loop:\n","            imgs = imgs.to(device)\n","            labels = labels.to(device).view(-1)\n","\n","            optimizer.zero_grad()\n","            outputs = model(imgs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * imgs.size(0)\n","            running_correct += (outputs.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","            # 프로그레스 바에 현재 배치 손실 정보 추가 (선택 사항)\n","            # train_loop.set_postfix(loss=loss.item())\n","\n","\n","        train_loss = running_loss / total\n","        training_loss_data[learning_rate].append(train_loss) # 에포크별 학습 손실 저장\n","        train_acc = running_correct / total\n","\n","        # 검증\n","        model.eval()\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","        # 검증 배치는 프로그레스 바 없이 진행\n","        with torch.no_grad():\n","            for imgs, labels in val_loader:\n","                imgs = imgs.to(device)\n","                labels = labels.to(device).view(-1)\n","                outputs = model(imgs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * imgs.size(0)\n","                val_correct += (outputs.argmax(1) == labels).sum().item()\n","                val_total += labels.size(0)\n","\n","        val_loss /= val_total\n","        val_acc = val_correct / val_total\n","\n","        # 에포크 결과 출력 (프로그레스 바 아래에 표시됨)\n","        print(f\"Epoch {epoch:2d} | \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n","              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","\n","    # 9. 평가 (각 학습률 학습 완료 후 평가)\n","    print(f\"\\n--- Evaluation with Learning Rate: {learning_rate} ---\")\n","    model.eval()\n","    test_correct = 0\n","    test_total = 0\n","    with torch.no_grad():\n","        for imgs, labels in test_loader:\n","            imgs = imgs.to(device)\n","            labels = labels.to(device).view(-1)\n","            outputs = model(imgs)\n","            preds = outputs.argmax(1)\n","            test_correct += (preds == labels).sum().item()\n","            test_total += labels.size(0)\n","\n","    print(f\"Test Accuracy (LR={learning_rate}): {test_correct / test_total:.4f}\")\n","\n","# 모든 학습률 실험 완료 후 최종 메시지\n","print(\"\\n--- All Learning Rate Experiments Completed ---\")"]},{"cell_type":"markdown","metadata":{"id":"c4bc32c2"},"source":["학습 과정에서 기록된 `training_loss_data`를 사용하여 에포크별 학습 손실 그래프를 그립니다. 각 라인은 다른 학습률에 해당됩니다."],"id":"c4bc32c2"},{"cell_type":"code","metadata":{"id":"c4a797a5"},"source":["import matplotlib.pyplot as plt\n","\n","# training_loss_data 딕셔너리가 학습 코드 실행 후 생성되었다고 가정합니다.\n","# 예시 데이터 구조: {learning_rate: [epoch1_loss, epoch2_loss, ...]}\n","# 만약 training_loss_data가 아직 없다면, 이전 학습 코드를 먼저 실행해야 합니다.\n","if 'training_loss_data' in locals() and training_loss_data:\n","    plt.figure(figsize=(12, 6))\n","\n","    for lr, losses in training_loss_data.items():\n","        epochs = range(1, len(losses) + 1)\n","        plt.plot(epochs, losses, marker='o', linestyle='-', label=f'LR: {lr}')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Training Loss')\n","    plt.title('Training Loss over Epochs for Different Learning Rates')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","else:\n","    print(\"training_loss_data를 찾을 수 없습니다. 먼저 학습 코드를 실행하여 데이터를 생성해주세요.\")"],"id":"c4a797a5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}